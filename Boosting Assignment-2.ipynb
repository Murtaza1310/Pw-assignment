{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q1. What is Gradient Boosting Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Boosting Regression is a machine learning technique used for regression tasks. It builds an additive model in a forward stage-wise fashion and allows for the optimization of arbitrary differentiable loss functions. In each stage, a regression tree is fit on the negative gradient of the given loss function. The technique produces a final prediction model in the form of an ensemble of weak predictors ¹. \n",
    "\n",
    "The `scikit-learn` library provides an implementation of Gradient Boosting Regression in Python. The `GradientBoostingRegressor` class in the `sklearn.ensemble` module can be used to build a Gradient Boosting Regression model. The class provides several hyperparameters that can be tuned to optimize the performance of the model ¹.\n",
    "\n",
    "Here is an example code snippet that demonstrates how to use the `GradientBoostingRegressor` class to build a Gradient Boosting Regression model:\n",
    "\n",
    "```python\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "# Create a Gradient Boosting Regression model\n",
    "model = GradientBoostingRegressor(loss='squared_error', learning_rate=0.1, n_estimators=100, subsample=1.0, criterion='friedman_mse', min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_depth=3, min_impurity_decrease=0.0, init=None, random_state=None, max_features=None, alpha=0.9, verbose=0, max_leaf_nodes=None, warm_start=False, validation_fraction=0.1, n_iter_no_change=None, tol=0.0001, ccp_alpha=0.0)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = model.predict(X_test)\n",
    "```\n",
    "\n",
    "Please note that the above code is just an example and the hyperparameters used in the `GradientBoostingRegressor` class may not be optimal for your specific use case. You may need to experiment with different hyperparameters to achieve the best performance for your problem ¹.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q2. Implement a simple gradient boosting algorithm from scratch using Python and NumPy. Use a wsimple regression problem as an example and train the model on a small dataset. Evaluate the model's performance using metrics such as mean squared error and R-squared."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sure, I can help you with that. Here is a simple implementation of Gradient Boosting Regression using Python and NumPy:\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "class GradientBoostingRegressor:\n",
    "    def __init__(self, n_estimators=100, learning_rate=0.1, max_depth=3):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_depth = max_depth\n",
    "        self.trees = []\n",
    "        self.intercept = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.intercept = np.mean(y)\n",
    "        y_pred = np.full_like(y, self.intercept)\n",
    "        for i in range(self.n_estimators):\n",
    "            residuals = y - y_pred\n",
    "            tree = DecisionTreeRegressor(max_depth=self.max_depth)\n",
    "            tree.fit(X, residuals)\n",
    "            self.trees.append(tree)\n",
    "            y_pred += self.learning_rate * tree.predict(X)\n",
    "\n",
    "    def predict(self, X):\n",
    "        y_pred = np.full(X.shape[0], self.intercept)\n",
    "        for tree in self.trees:\n",
    "            y_pred += self.learning_rate * tree.predict(X)\n",
    "        return y_pred\n",
    "```\n",
    "\n",
    "This implementation uses the `DecisionTreeRegressor` class from the `sklearn.tree` module to build regression trees. The `fit` method trains the model on the input data `X` and target values `y`. The `predict` method makes predictions on new data.\n",
    "\n",
    "To evaluate the performance of the model, you can use metrics such as mean squared error and R-squared. Here is an example code snippet that demonstrates how to calculate these metrics:\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Train the model\n",
    "model = GradientBoostingRegressor()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate mean squared error\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "# Calculate R-squared\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q3. Experiment with different hyperparameters such as learning rate, number of trees, and tree depth to optimise the performance of the model. Use grid search or random search to find the best hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To answer this question, you need to write some code in Python and NumPy that implements a simple gradient boosting algorithm and uses grid search or random search to find the best hyperparameters. Here is a possible outline of the code:\n",
    "\n",
    "# Import libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "\n",
    "# Define a simple regression problem\n",
    "# For example, generate some data from a quadratic function with some noise\n",
    "np.random.seed(0)\n",
    "X = np.linspace(-5, 5, 100).reshape(-1, 1)\n",
    "y = X**2 + 10*np.random.randn(100)\n",
    "\n",
    "# Plot the data\n",
    "plt.scatter(X, y)\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "plt.show()\n",
    "\n",
    "# Define a simple decision tree as the base learner\n",
    "# For example, use the DecisionTreeRegressor from sklearn.tree\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "base_learner = DecisionTreeRegressor(max_depth=1)\n",
    "\n",
    "# Define a function to fit a gradient boosting model\n",
    "def gradient_boosting(X, y, base_learner, n_estimators, learning_rate):\n",
    "  # Initialize the model\n",
    "  model = []\n",
    "  # Initialize the prediction\n",
    "  y_pred = np.zeros_like(y)\n",
    "  # Loop over the number of estimators\n",
    "  for i in range(n_estimators):\n",
    "    # Fit the base learner on the negative gradient (residuals)\n",
    "    base_learner.fit(X, y - y_pred)\n",
    "    # Append the base learner to the model\n",
    "    model.append(base_learner)\n",
    "    # Update the prediction by adding the scaled base learner prediction\n",
    "    y_pred += learning_rate * base_learner.predict(X)\n",
    "  # Return the model and the final prediction\n",
    "  return model, y_pred\n",
    "\n",
    "# Define a function to evaluate a gradient boosting model\n",
    "def evaluate_model(y_true, y_pred):\n",
    "  # Calculate the mean squared error\n",
    "  mse = mean_squared_error(y_true, y_pred)\n",
    "  # Calculate the R-squared score\n",
    "  r2 = r2_score(y_true, y_pred)\n",
    "  # Print the results\n",
    "  print(f'MSE: {mse:.2f}')\n",
    "  print(f'R2: {r2:.2f}')\n",
    "\n",
    "# Define the hyperparameters to search over\n",
    "# For example, use a dictionary with keys as the hyperparameter names and values as the lists of possible values\n",
    "hyperparameters = {\n",
    "  'n_estimators': [10, 20, 50, 100],\n",
    "  'learning_rate': [0.01, 0.1, 0.5, 1.0]\n",
    "}\n",
    "\n",
    "# Choose a search method\n",
    "# For example, use grid search or random search from sklearn.model_selection\n",
    "# Grid search will try all possible combinations of the hyperparameters\n",
    "# Random search will try a random subset of the hyperparameters\n",
    "search_method = GridSearchCV # or RandomizedSearchCV\n",
    "\n",
    "# Create a search object\n",
    "# For example, use the search method with the gradient boosting function, the hyperparameters, and a scoring metric\n",
    "search = search_method(gradient_boosting, hyperparameters, scoring='neg_mean_squared_error')\n",
    "\n",
    "# Fit the search object on the data\n",
    "search.fit(X, y)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_hyperparameters = search.best_params_\n",
    "print(f'Best hyperparameters: {best_hyperparameters}')\n",
    "\n",
    "# Get the best model and the best prediction\n",
    "best_model, best_pred = search.best_estimator_\n",
    "\n",
    "# Evaluate the best model\n",
    "evaluate_model(y, best_pred)\n",
    "\n",
    "# Plot the best prediction\n",
    "plt.scatter(X, y, label='True data')\n",
    "plt.plot(X, best_pred, color='red', label='Best prediction')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q4. What is a weak learner in Gradient Boosting?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A weak learner in Gradient Boosting is a simple model that performs only slightly better than random guessing. It is usually a shallow decision tree that has a few splits and a low predictive power. The idea of Gradient Boosting is to combine many weak learners into a strong learner, by training each new weak learner to focus on the errors of the previous ones. The final prediction is given by the weighted sum of the weak learners' predictions ¹²³⁴⁵."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q5. What is the intuition behind the Gradient Boosting algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The intuition behind the Gradient Boosting algorithm is to combine many weak learners into a strong learner, by training each new weak learner to focus on the errors of the previous ones. The final prediction is given by the weighted sum of the weak learners' predictions ¹.\n",
    "\n",
    "The algorithm works as follows:\n",
    "\n",
    "- First, an initial model (usually a constant value) is fitted to the data and the residuals (the difference between the true and predicted values) are computed.\n",
    "- Then, a weak learner (usually a shallow decision tree) is fitted to the residuals, and its predictions are added to the initial model with a learning rate (a small positive factor that controls the contribution of each weak learner).\n",
    "- The residuals are updated by subtracting the scaled weak learner predictions from the previous residuals.\n",
    "- The process is repeated until a predefined number of weak learners are added or the residuals are minimized.\n",
    "\n",
    "The algorithm can be applied to both regression and classification problems, by using different loss functions to measure the residuals. For example, for regression problems, the mean squared error or the absolute error can be used as the loss function. For classification problems, the binomial deviance or the exponential loss can be used as the loss function ²³⁴⁵."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q6. How does Gradient Boosting algorithm build an ensemble of weak learners?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Gradient Boosting algorithm builds an ensemble of weak learners by training them sequentially and iteratively. A weak learner is a simple model that performs slightly better than random guessing, such as a shallow decision tree. The algorithm works as follows ¹²³:\n",
    "\n",
    "- First, an initial model (usually a constant value) is fitted to the data and the residuals (the difference between the true and predicted values) are computed.\n",
    "- Then, a weak learner is fitted to the residuals, and its predictions are added to the initial model with a learning rate (a small positive factor that controls the contribution of each weak learner).\n",
    "- The residuals are updated by subtracting the scaled weak learner predictions from the previous residuals.\n",
    "- The process is repeated until a predefined number of weak learners are added or the model's performance has plateaued.\n",
    "\n",
    "The algorithm can be applied to both regression and classification problems, by using different loss functions to measure the residuals. For example, for regression problems, the mean squared error or the absolute error can be used as the loss function. For classification problems, the binomial deviance or the exponential loss can be used as the loss function ²³⁴⁵."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q7. What are the steps involved in constructing the mathematical intuition of Gradient Boosting algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The steps involved in constructing the mathematical intuition of Gradient Boosting algorithm are as follows ¹:\n",
    "\n",
    "- Start with an initial model that predicts a constant value for all samples. For regression problems, this value is usually the mean of the target variable. For classification problems, this value is usually the log-odds of the target class.\n",
    "- Compute the residuals, which are the differences between the actual and predicted values of the target variable.\n",
    "- Fit a weak learner, such as a shallow decision tree, to the residuals. This weak learner tries to capture the patterns in the residuals that the initial model missed.\n",
    "- Update the predictions by adding the scaled weak learner predictions to the previous predictions. The scaling factor is the learning rate, which controls the contribution of each weak learner to the final model.\n",
    "- Repeat steps 2 to 4 until a predefined number of weak learners are added or the residuals are minimized.\n",
    "\n",
    "The final model is the weighted sum of the weak learners' predictions. The loss function, which measures the discrepancy between the actual and predicted values, can be chosen according to the problem type. For example, for regression problems, the mean squared error or the absolute error can be used as the loss function. For classification problems, the binomial deviance or the exponential loss can be used as the loss function ²³⁴⁵."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
