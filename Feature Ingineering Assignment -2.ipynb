{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1. What is Min-Max scaling, and how is it used in data preprocessing? Provide an example to illustrate its application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Min-Max scaling** is a data preprocessing technique that scales and translates each feature individually to a given range. It is often used to transform features to a range between zero and one on the training set¹. The transformation is given by the following formula:\n",
    "\n",
    "\\[\n",
    "X_{\\text{std}} = \\frac{{X - X_{\\text{min}}}}{{X_{\\text{max}} - X_{\\text{min}}}}\n",
    "\\]\n",
    "\n",
    "\\[\n",
    "X_{\\text{scaled}} = X_{\\text{std}} \\times (\\text{{max}} - \\text{{min}}) + \\text{{min}}\n",
    "\\]\n",
    "\n",
    "where \\(X_{\\text{min}}\\) and \\(X_{\\text{max}}\\) are the minimum and maximum values of the feature, respectively¹.\n",
    "\n",
    "This technique is often used as an alternative to zero mean, unit variance scaling¹. It does not reduce the effect of outliers but linearly scales them down into a fixed range, where the largest occurring data point corresponds to the maximum value, and the smallest one corresponds to the minimum value¹.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MinMaxScaler()\n",
      "[[0.   0.  ]\n",
      " [0.25 0.25]\n",
      " [0.5  0.5 ]\n",
      " [1.   1.  ]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "## Here's an example to illustrate its application:\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "data = [[-1, 2], [-0.5, 6], [0, 10], [1, 18]]\n",
    "scaler = MinMaxScaler()\n",
    "print(scaler.fit(data))\n",
    "print(scaler.transform(data))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2. What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling? Provide an example to illustrate its application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **Unit Vector technique** in feature scaling is a method that scales each feature individually to have a **unit norm**. It involves dividing each feature vector by its norm, which can be either the **Manhattan distance** (L1 norm) or the **Euclidean distance** (L2 norm) of the vector².\n",
    "\n",
    "When scaling to the unit norm, each observation vector is divided by its norm, resulting in a feature vector with a length of one². This technique is useful when dealing with features that have hard boundaries, such as image data, where colors can range from 0 to 255⁴.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.4472136   0.89442719]\n",
      " [-0.08304548  0.99654576]\n",
      " [ 0.          1.        ]\n",
      " [ 0.05547002  0.99846035]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "data = [[-1, 2], [-0.5, 6], [0, 10], [1, 18]]\n",
    "scaler = Normalizer(norm='l2')\n",
    "print(scaler.fit_transform(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3. What is PCA (Principle Component Analysis), and how is it used in dimensionality reduction? Provide an example to illustrate its application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Principal Component Analysis (PCA)** is a statistical technique used for **dimensionality reduction**. It identifies a set of orthogonal axes, called **principal components**, that capture the maximum variance in the data¹. The principal components are linear combinations of the original variables in the dataset and are ordered in decreasing order of importance¹.\n",
    "\n",
    "PCA is widely used in exploratory data analysis and machine learning for predictive models¹. It helps reduce the dimensionality of a dataset while preserving the most important patterns or relationships between the variables without any prior knowledge of the target variables¹. By reducing the number of input features, PCA can address issues such as overfitting, increased computation time, and reduced accuracy caused by the curse of dimensionality¹.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-7.05447553]\n",
      " [-3.02334666]\n",
      " [ 1.00778222]\n",
      " [ 9.07003997]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "data = [[-1, 2], [-0.5, 6], [0, 10], [1, 18]]\n",
    "pca = PCA(n_components=1)\n",
    "print(pca.fit_transform(data))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q4. What is the relationship between PCA and Feature Extraction, and how can PCA be used for Feature Extraction? Provide an example to illustrate this concept."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Principal Component Analysis (PCA)** is a statistical technique used for **dimensionality reduction**¹. It identifies a set of orthogonal axes, called **principal components**, that capture the maximum variance in the data¹. PCA is widely used in exploratory data analysis and machine learning for predictive models¹.\n",
    "\n",
    "Feature extraction is a process of **dimensionality reduction** where an initial set of raw data is reduced to more manageable groups for processing³. PCA is one of the most popular dimensionality reduction techniques used for feature extraction¹. It aims to reduce the number of input features while retaining as much of the original information as possible¹.\n",
    "\n",
    "The relationship between PCA and feature extraction lies in the fact that PCA can be used as a technique for feature extraction¹. By transforming the original features into a new set of variables, smaller than the original set, PCA retains most of the sample's information and is useful for regression and classification tasks¹.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-7.05447553]\n",
      " [-3.02334666]\n",
      " [ 1.00778222]\n",
      " [ 9.07003997]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "data = [[-1, 2], [-0.5, 6], [0, 10], [1, 18]]\n",
    "pca = PCA(n_components=1)\n",
    "print(pca.fit_transform(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q5. You are working on a project to build a recommendation system for a food delivery service. The dataset contains features such as price, rating, and delivery time. Explain how you would use Min-Max scaling to preprocess the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To preprocess the dataset for building a recommendation system for a food delivery service, you can use **Min-Max scaling** to transform the features such as price, rating, and delivery time to a specific range¹².\n",
    "\n",
    "Min-Max scaling is a technique that scales and translates each feature individually to a given range, often between zero and one¹. It can be performed using the `MinMaxScaler` class from the scikit-learn library¹. The transformation is given by the following formula:\n",
    "\n",
    "\\[\n",
    "X_{\\text{std}} = \\frac{{X - X_{\\text{min}}}}{{X_{\\text{max}} - X_{\\text{min}}}}\n",
    "\\]\n",
    "\n",
    "\\[\n",
    "X_{\\text{scaled}} = X_{\\text{std}} \\times (\\text{{max}} - \\text{{min}}) + \\text{{min}}\n",
    "\\]\n",
    "\n",
    "where \\(X_{\\text{min}}\\) and \\(X_{\\text{max}}\\) are the minimum and maximum values of the feature, respectively¹.\n",
    "\n",
    "By applying Min-Max scaling to the dataset, you can ensure that each feature is transformed to a range between zero and one¹. This normalization process can help prevent features with larger values from dominating the recommendation system's calculations¹.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.28571429 0.77777778 0.4       ]\n",
      " [1.         0.         1.        ]\n",
      " [0.         0.44444444 0.        ]\n",
      " [0.57142857 1.         0.6       ]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "## Here's an example of how you can use Min-Max scaling with the scikit-learn library:\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Assuming you have a dataset with features: price, rating, and delivery time\n",
    "data = [[10.0, 4.5, 30], [15.0, 3.8, 45], [8.0, 4.2, 20], [12.0, 4.7, 35]]\n",
    "\n",
    "# Create an instance of MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Fit the scaler on the data\n",
    "scaler.fit(data)\n",
    "\n",
    "# Transform the data using Min-Max scaling\n",
    "scaled_data = scaler.transform(data)\n",
    "\n",
    "print(scaled_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q6. You are working on a project to build a model to predict stock prices. The dataset contains many features, such as company financial data and market trends. Explain how you would use PCA to reduce the dimensionality of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Principal Component Analysis (PCA)** is a statistical technique used for **dimensionality reduction**¹. It identifies a set of orthogonal axes, called **principal components**, that capture the maximum variance in the data¹. PCA is widely used in exploratory data analysis and machine learning for predictive models¹.\n",
    "\n",
    "When working on a project to predict stock prices, you can use PCA to reduce the dimensionality of the dataset containing features such as company financial data and market trends¹.\n",
    "\n",
    "The process of using PCA for dimensionality reduction involves the following steps:\n",
    "\n",
    "1. **Data Preprocessing**: Ensure that the dataset is properly preprocessed by handling missing values, normalizing or standardizing features, and addressing any other data quality issues.\n",
    "\n",
    "2. **Feature Selection**: Identify the relevant features from the dataset that are most likely to contribute to predicting stock prices. This step helps reduce computational complexity and focuses on the most informative features.\n",
    "\n",
    "3. **Applying PCA**: Apply PCA to the selected features to reduce their dimensionality while retaining most of the original information. PCA transforms the original features into a new set of uncorrelated variables called principal components¹. These principal components are ordered in decreasing order of importance, with the first component capturing the most variance in the data.\n",
    "\n",
    "4. **Determining the Number of Components**: Determine the number of principal components to retain based on a trade-off between computational efficiency and information loss. You can consider using metrics such as explained variance ratio or scree plot analysis to make an informed decision¹.\n",
    "\n",
    "5. **Model Training**: Train your predictive model using the reduced-dimensional dataset obtained after applying PCA. The reduced dataset should contain a subset of principal components that capture most of the variance in the original dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'feature_1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Murtaza\\Desktop\\MAchine LEarning Assignment\\Feature Ingineering Assignment -2.ipynb Cell 18\u001b[0m line \u001b[0;36m5\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Murtaza/Desktop/MAchine%20LEarning%20Assignment/Feature%20Ingineering%20Assignment%20-2.ipynb#X30sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdecomposition\u001b[39;00m \u001b[39mimport\u001b[39;00m PCA\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Murtaza/Desktop/MAchine%20LEarning%20Assignment/Feature%20Ingineering%20Assignment%20-2.ipynb#X30sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# Assuming you have a dataset with multiple features\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Murtaza/Desktop/MAchine%20LEarning%20Assignment/Feature%20Ingineering%20Assignment%20-2.ipynb#X30sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m data \u001b[39m=\u001b[39m [[feature_1, feature_2, \u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m, feature_n], \u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m]\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Murtaza/Desktop/MAchine%20LEarning%20Assignment/Feature%20Ingineering%20Assignment%20-2.ipynb#X30sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39m# Create an instance of PCA with desired number of components\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Murtaza/Desktop/MAchine%20LEarning%20Assignment/Feature%20Ingineering%20Assignment%20-2.ipynb#X30sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m pca \u001b[39m=\u001b[39m PCA(n_components\u001b[39m=\u001b[39mk)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'feature_1' is not defined"
     ]
    }
   ],
   "source": [
    "## Here's an example illustrating how you can use PCA for dimensionality reduction in Python:\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Assuming you have a dataset with multiple features\n",
    "data = [[feature_1, feature_2, ..., feature_n], ...]\n",
    "\n",
    "# Create an instance of PCA with desired number of components\n",
    "pca = PCA(n_components=k)\n",
    "\n",
    "# Fit PCA on your dataset\n",
    "pca.fit(data)\n",
    "\n",
    "# Transform your dataset using PCA\n",
    "reduced_data = pca.transform(data)\n",
    "\n",
    "## This is just an example f n features so there is not output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q7. For a dataset containing the following values: [1, 5, 10, 15, 20], perform Min-Max scaling to transform the values to a range of -1 to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "Data = [[1,5,10,15,20]]\n",
    "\n",
    "min_max= MinMaxScaler()\n",
    "\n",
    "min_max.fit_transform(Data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q8. For a dataset containing the following features: [height, weight, age, gender, blood pressure], perform Feature Extraction using PCA. How many principal components would you choose to retain, and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When performing **Feature Extraction using PCA** on a dataset with features such as height, weight, age, gender, and blood pressure, the number of principal components to retain depends on the desired trade-off between **dimensionality reduction** and **information preservation**¹.\n",
    "\n",
    "To determine the number of principal components to retain, you can consider the following factors:\n",
    "\n",
    "1. **Explained Variance Ratio**: Calculate the explained variance ratio for each principal component. The explained variance ratio represents the proportion of the dataset's variance explained by each principal component. Retaining principal components with high explained variance ratios ensures that most of the original information is preserved¹.\n",
    "\n",
    "2. **Cumulative Explained Variance**: Plot the cumulative explained variance ratio against the number of principal components. This plot helps visualize how much of the dataset's variance is preserved as the number of principal components increases. You can choose a threshold for the cumulative explained variance (e.g., 90% or 95%) and select the minimum number of principal components that exceed this threshold¹.\n",
    "\n",
    "3. **Domain Knowledge**: Consider any domain-specific knowledge or requirements that may influence the choice of principal components to retain. For example, certain features may be more relevant or informative for your specific application, and you may want to prioritize their preservation¹."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
