{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q1. What is the KNN algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KNN stands for **K-Nearest Neighbors**. It is a **supervised learning algorithm** used for **classification** and **regression** problems. The algorithm works by finding the K closest data points in the training dataset to a new data point and then predicts the label or value of the new data point based on the majority vote or average of the K neighbors ¹. \n",
    "\n",
    "The KNN algorithm is widely used in **pattern recognition**, **data mining**, and **intrusion detection**. It is a **non-parametric** method that does not make any underlying assumptions about the distribution of data, making it a flexible choice for various types of datasets in classification and regression tasks ¹². \n",
    "\n",
    "The algorithm is based on the concept of similarity between data points. It predicts the label or value of a new data point by considering its K closest neighbors in the training dataset. The distance metric used to calculate the similarity between data points can be **Euclidean distance**, **Manhattan distance**, or **Minkowski distance** ¹. \n",
    "\n",
    "The KNN algorithm is easy to understand and implement, making it a popular choice for beginners in machine learning. However, it can be computationally expensive for large datasets and requires careful selection of the value of K ¹³. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q2. How do you choose the value of K in KNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The value of K in the KNN algorithm is an important hyperparameter that can affect the performance of the model. Choosing the right value of K is crucial to ensure that the model generalizes well to new data.\n",
    "\n",
    "There are several methods to choose the value of K in KNN, including:\n",
    "\n",
    "1. **Elbow method**: This method involves plotting the accuracy of the model against different values of K and selecting the value of K at which the accuracy starts to plateau. This value of K is chosen as the optimal value ¹.\n",
    "\n",
    "2. **Cross-validation**: This method involves dividing the dataset into training and validation sets and evaluating the model's performance for different values of K. The value of K that gives the best performance on the validation set is chosen as the optimal value ².\n",
    "\n",
    "3. **Domain knowledge**: The value of K can also be chosen based on domain knowledge or prior experience with similar datasets. For example, if the dataset has a large number of outliers, a higher value of K may be more appropriate ³.\n",
    "\n",
    "It is generally recommended to choose an odd value of K to avoid ties in classification ¹⁴. Another simple approach to select K is to set K = sqrt(n), where n is the number of data points in the training dataset ²."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q3. What is the difference between KNN classifier and KNN regressor?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The KNN algorithm can be used for both classification and regression tasks. The main difference between KNN classifier and KNN regressor is the type of output they produce.\n",
    "\n",
    "KNN classifier is used for **classification** tasks, where the goal is to predict the class or category of a new data point based on its similarity to the K nearest neighbors in the training dataset. The output of KNN classifier is a **discrete** class label ¹².\n",
    "\n",
    "KNN regressor, on the other hand, is used for **regression** tasks, where the goal is to predict the value of a continuous variable for a new data point based on its similarity to the K nearest neighbors in the training dataset. The output of KNN regressor is a **continuous** value that represents the average or median of the target variable of the K nearest neighbors ¹³.\n",
    "\n",
    "In summary, KNN classifier is used for classification tasks and produces a discrete class label, while KNN regressor is used for regression tasks and produces a continuous value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q4. How do you measure the performance of KNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performance of the KNN algorithm can be evaluated using various metrics depending on the type of task, such as **classification accuracy**, **precision**, **recall**, **F1 score**, and **mean squared error** for regression tasks  .\n",
    "\n",
    "For classification tasks, the most commonly used metric is **classification accuracy**, which measures the proportion of correctly classified instances in the test dataset. It is defined as the ratio of the number of correctly classified instances to the total number of instances in the test dataset . \n",
    "\n",
    "Other metrics such as **precision**, **recall**, and **F1 score** can be used to evaluate the performance of the model for imbalanced datasets or when the cost of false positives and false negatives is different .\n",
    "\n",
    "For regression tasks, the most commonly used metric is **mean squared error (MSE)**, which measures the average squared difference between the predicted and actual values of the target variable in the test dataset .\n",
    "\n",
    "In practice, it is recommended to use a combination of metrics to evaluate the performance of the model and choose the one that is most appropriate for the specific task and dataset  ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q5. What is the curse of dimensionality in KNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The curse of dimensionality is a problem that KNN faces when the number of features increases. KNN performs best with a low number of features because when there are more features, it requires more data, which creates an overfitting problem. The curse of dimensionality also describes the phenomenon where the feature space becomes increasingly sparse for an increasing number of dimensions of a fixed-size training dataset. KNN is very susceptible to overfitting due to the curse of dimensionality ¹.\n",
    "\n",
    "In high-dimensional spaces, the KNN algorithm faces two difficulties: it becomes computationally more expensive to compute distance and find the nearest neighbors in high-dimensional space, and as the number of features increases, the distance between data points becomes less distinctive ³⁴. \n",
    "\n",
    "To overcome the curse of dimensionality, several techniques can be used, such as **dimensionality reduction**, **feature selection**, and **feature extraction**. These techniques aim to reduce the number of features in the dataset while preserving the most relevant information ²."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q6. How do you handle missing values in KNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To handle missing values in KNN, you can use the `KNNImputer` class from the `sklearn.impute` module ¹. Here's how it works:\n",
    "\n",
    "1. Load `KNNImputer` from `sklearn.impute`:\n",
    "```python\n",
    "from sklearn.impute import KNNImputer\n",
    "```\n",
    "\n",
    "2. Initialize `KNNImputer`. You can define your own `n_neighbors` value (as its typical of KNN algorithm):\n",
    "```python\n",
    "imputer = KNNImputer(n_neighbors=2)\n",
    "```\n",
    "\n",
    "3. Impute/Fill Missing Values. Each sample’s missing values are imputed using the mean value from `n_neighbors` nearest neighbors found in the training set. Two samples are close if the features that neither is missing are close. By default, a euclidean distance metric that supports missing values, `nan_euclidean_distances`, is used to find the nearest neighbors:\n",
    "```python\n",
    "X = [[1, 2, np.nan], [3, 4, 5], [np.nan, 6, 7], [8, 9, 10]]\n",
    "imputer.fit_transform(X)\n",
    "```\n",
    "Output:\n",
    "```python\n",
    "array([[1., 2., 6.],\n",
    "       [3., 4., 5.],\n",
    "       [4., 6., 7.],\n",
    "       [8., 9., 10.]])\n",
    "```\n",
    "\n",
    "Alternatively, you can use other imputation methods such as **mean imputation**, **median imputation**, or **mode imputation** to handle missing values in KNN ²."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q7. Compare and contrast the performance of the KNN classifier and regressor. Which one is better for which type of problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The KNN algorithm can be used for both classification and regression tasks. The main difference between KNN classifier and KNN regressor is the type of output they produce.\n",
    "\n",
    "KNN classifier is used for **classification** tasks, where the goal is to predict the class or category of a new data point based on its similarity to the K nearest neighbors in the training dataset. The output of KNN classifier is a **discrete** class label ¹².\n",
    "\n",
    "KNN regressor, on the other hand, is used for **regression** tasks, where the goal is to predict the value of a continuous variable for a new data point based on its similarity to the K nearest neighbors in the training dataset. The output of KNN regressor is a **continuous** value that represents the average or median of the target variable of the K nearest neighbors ¹³.\n",
    "\n",
    "In terms of performance, the choice between KNN classifier and KNN regressor depends on the type of problem and the nature of the data. KNN classifier is generally better suited for **categorical** data, where the output variable is a discrete class label. KNN regressor, on the other hand, is better suited for **continuous** data, where the output variable is a numerical value ⁴.\n",
    "\n",
    "It is important to note that the performance of KNN classifier and KNN regressor depends on several factors such as the value of K, the distance metric used, and the quality of the training data. It is recommended to experiment with different values of K and distance metrics and evaluate the performance of the model using appropriate metrics such as accuracy, precision, recall, F1 score, or mean squared error ⁵."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q8. What are the strengths and weaknesses of the KNN algorithm for classification and regression tasks, and how can these be addressed?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The KNN algorithm has several strengths and weaknesses for classification and regression tasks.\n",
    "\n",
    "**Strengths:**\n",
    "- KNN is a simple and easy-to-understand algorithm that does not require any training, making it a popular choice for beginners in machine learning ¹.\n",
    "- KNN can handle both numerical and categorical data, making it a flexible choice for various types of datasets in classification and regression tasks ².\n",
    "- KNN is robust to noisy data, as it relies on the majority vote or average of the K nearest neighbors, making it less sensitive to outliers compared to other algorithms ³.\n",
    "- KNN is a non-parametric method that does not make any underlying assumptions about the distribution of data, making it a flexible choice for various types of datasets in classification and regression tasks ⁴.\n",
    "\n",
    "**Weaknesses:**\n",
    "- KNN can be computationally expensive for large datasets, as it requires calculating the distance between each data point in the training dataset and the new data point .\n",
    "- KNN is sensitive to the value of K, which can affect the performance of the model. Choosing the right value of K is crucial to ensure that the model generalizes well to new data .\n",
    "- KNN is sensitive to the distance metric used to calculate the similarity between data points. Choosing the right distance metric is important to ensure that the model captures the underlying patterns in the data .\n",
    "- KNN is susceptible to the curse of dimensionality, which is a problem that arises when the number of features increases. To overcome this problem, several techniques such as dimensionality reduction, feature selection, and feature extraction can be used .\n",
    "\n",
    "To address the weaknesses of KNN, several techniques can be used, such as:\n",
    "- Using **approximate nearest neighbor** algorithms to speed up the computation of distances between data points .\n",
    "- Using **cross-validation** to choose the optimal value of K and the distance metric  .\n",
    "- Using **dimensionality reduction** techniques such as **PCA** or **t-SNE** to reduce the number of features in the dataset while preserving the most relevant information ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q9. What is the difference between Euclidean distance and Manhattan distance in KNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Euclidean distance and Manhattan distance are two commonly used distance metrics in the KNN algorithm.\n",
    "\n",
    "The **Euclidean distance** is the straight-line distance between two points in a Euclidean space. It is calculated as the square root of the sum of the squared differences between the coordinates of two points. The Euclidean distance is given by the following formula:\n",
    "\n",
    "$$d_{E}(p,q) = \\sqrt{\\sum_{i=1}^{n}(q_{i}-p_{i})^{2}}$$\n",
    "\n",
    "where `p` and `q` are two points in the n-dimensional space.\n",
    "\n",
    "The **Manhattan distance** is the sum of the absolute differences between the coordinates of two points. It is calculated as the sum of the absolute differences between the x-coordinates and y-coordinates of two points. The Manhattan distance is given by the following formula:\n",
    "\n",
    "$$d_{M}(p,q) = \\sum_{i=1}^{n}|q_{i}-p_{i}|$$\n",
    "\n",
    "where `p` and `q` are two points in the n-dimensional space.\n",
    "\n",
    "The main difference between the two distance metrics is the way they measure distance. The Euclidean distance measures the shortest distance between two points, while the Manhattan distance measures the distance between two points along the axes at right angles. The Euclidean distance is more sensitive to outliers than the Manhattan distance, as it takes into account the squared differences between the coordinates of two points. The Manhattan distance, on the other hand, is less sensitive to outliers, as it only takes into account the absolute differences between the coordinates of two points.\n",
    "\n",
    "In practice, the choice between the two distance metrics depends on the nature of the data and the specific problem. The Euclidean distance is more appropriate for continuous data, while the Manhattan distance is more appropriate for discrete data or data with a large number of categorical variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q10. What is the role of feature scaling in KNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The role of feature scaling in KNN is to prevent features with larger magnitudes from dominating the distance calculations. Feature scaling is a process of normalizing the range of independent variables in the dataset. It is crucial for the KNN algorithm, as it is a distance-based algorithm that relies on measuring distances like Euclidean distance between different points. Scaling all features to a common scale gives each feature an equal weight in distance calculations ¹³.\n",
    "\n",
    "When the features have different scales, the distance metric used in KNN can be biased towards features with larger magnitudes, leading to poor performance of the model. Feature scaling ensures that all features contribute equally to the model and avoids the domination of features with larger values ².\n",
    "\n",
    "There are several methods for feature scaling, including **min-max scaling**, **standardization**, and **normalization**. Min-max scaling scales the features to a fixed range, typically between 0 and 1. Standardization scales the features to have zero mean and unit variance. Normalization scales the features to have unit norm ⁴.\n",
    "\n",
    "In practice, the choice of feature scaling method depends on the nature of the data and the specific problem. It is recommended to experiment with different scaling methods and evaluate the performance of the model using appropriate metrics such as accuracy, precision, recall, F1 score, or mean squared error ⁵."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
