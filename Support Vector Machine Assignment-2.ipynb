{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q1. What is the relationship between polynomial functions and kernel functions in machine learning algorithms?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In machine learning, a **kernel function** is a mathematical function that transforms data into a higher-dimensional feature space, where it becomes easier to separate the data points using a linear classifier ¹. A **polynomial function** is a function that involves one or more terms in which the variable is raised to a power and multiplied by a coefficient ². \n",
    "\n",
    "In SVMs, the **polynomial kernel** is a kernel function that represents the similarity of vectors in a feature space over polynomials of the original variables, allowing learning of non-linear models ³. The polynomial kernel introduces a layer of complexity by applying polynomial transformations to the data points, and is designed to handle situations where a simple linear separation isn’t sufficient ¹. Polynomial kernels are similar to linear kernels, but they can capture more complex patterns in the data ¹.\n",
    "\n",
    "In summary, polynomial functions and kernel functions are related in machine learning algorithms through the use of polynomial kernels, which transform the data into a higher-dimensional feature space using polynomial functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q2. How can we implement an SVM with a polynomial kernel in Python using Scikit-learn?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To implement an SVM with a polynomial kernel in Python using Scikit-learn, you can use the `SVC` class from the `sklearn.svm` module. Here's an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the model on the testing set is 0.98\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the iris dataset\n",
    "iris = load_iris()\n",
    "\n",
    "# Split the dataset into a training set and a testing set\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train an SVM with a polynomial kernel on the training set\n",
    "clf = SVC(kernel='poly', degree=3, gamma='scale', C=1.0)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels for the testing set\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Compute the accuracy of the model on the testing set\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"The accuracy of the model on the testing set is {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we first load the iris dataset using the `load_iris()` function from the `sklearn.datasets` module. We then split the dataset into a training set and a testing set using the `train_test_split()` function from the `sklearn.model_selection` module. We train an SVM with a polynomial kernel on the training set using the `SVC()` function from the `sklearn.svm` module, with the `kernel` parameter set to `'poly'`, the `degree` parameter set to `3`, the `gamma` parameter set to `'scale'`, and the `C` parameter set to `1.0`. We predict the labels for the testing set using the `predict()` method, compute the accuracy of the model on the testing set using the `accuracy_score()` function from the `sklearn.metrics` module, and print the accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q3. How does increasing the value of epsilon affect the number of support vectors in SVR?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In **Support Vector Regression (SVR)**, the **epsilon** parameter defines the width of the tube around the regression line within which no penalty is associated in the training loss function ¹. The number of support vectors in SVR is affected by the value of epsilon. Increasing the value of epsilon will result in more data points being included in the tube, which means that more data points will be classified as support vectors ². Conversely, decreasing the value of epsilon will result in fewer data points being included in the tube, which means that fewer data points will be classified as support vectors ². \n",
    "\n",
    "In summary, increasing the value of epsilon in SVR will generally increase the number of support vectors, while decreasing the value of epsilon will generally decrease the number of support vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q4. How does the choice of kernel function, C parameter, epsilon parameter, and gamma parameter affect the performance of Support Vector Regression (SVR)? Can you explain how each parameter works and provide examples of when you might want to increase or decrease its value?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In **Support Vector Regression (SVR)**, the choice of kernel function, C parameter, epsilon parameter, and gamma parameter can significantly affect the performance of the model ¹. Here's how each parameter works:\n",
    "\n",
    "- **Kernel function**: The kernel function is used to transform the input data into a higher-dimensional feature space, where it becomes easier to separate the data points using a linear classifier. The choice of kernel function depends on the characteristics of the data and the complexity of the task. Some common kernel functions include linear, polynomial, radial basis function (RBF), and sigmoid ².\n",
    "\n",
    "- **C parameter**: The C parameter controls the trade-off between achieving a low training error and minimizing the norm of the weights in SVMs with a linear kernel. A small value of C will cause the optimizer to look for a larger-margin separating hyperplane, even if that hyperplane misclassifies more points. Conversely, a very large value of C will cause the optimizer to choose a smaller-margin hyperplane if that hyperplane does a better job of getting all the training points classified correctly ³.\n",
    "\n",
    "- **Epsilon parameter**: The epsilon parameter specifies the width of the tube around the regression line within which no penalty is associated in the training loss function. A larger value of epsilon will result in more data points being included in the tube, which means that more data points will be classified as support vectors. Conversely, a smaller value of epsilon will result in fewer data points being included in the tube, which means that fewer data points will be classified as support vectors ⁴.\n",
    "\n",
    "- **Gamma parameter**: The gamma parameter controls the shape of the decision boundary in SVMs with a radial basis function (RBF) kernel. A small value of gamma will result in a more flexible decision boundary, while a large value of gamma will result in a more rigid decision boundary ⁵.\n",
    "\n",
    "The optimal values for these parameters depend on the characteristics of the data and the complexity of the task. In general, increasing the value of C will make the model more complex and more likely to overfit the training data, while decreasing the value of C will make the model less complex and more likely to underfit the training data. Similarly, increasing the value of epsilon will make the model more complex and more likely to overfit the training data, while decreasing the value of epsilon will make the model less complex and more likely to underfit the training data. The optimal value of gamma depends on the scale of the input data, and should be chosen using cross-validation ⁶."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q5. Assignment:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Import the necessary libraries and load the dataset\n",
    "\n",
    "- Split the dataset into training and testing setZ\n",
    "\n",
    "- Preprocess the data using any technique of your choice (e.g. scaling, normaliMationK)\n",
    "\n",
    "- Create an instance of the SVC classifier and train it on the training datW\n",
    "\n",
    "- hse the trained classifier to predict the labels of the testing datW\n",
    "\n",
    "- Evaluate the performance of the classifier using any metric of your choice (e.g. accuracy, precision, recall, F1-score)\n",
    "\n",
    "- Tune the hyperparameters of the SVC classifier using GridSearchCV or RandomiMedSearchCV to improve its performanc_\n",
    "\n",
    "- Train the tuned classifier on the entire dataseg\n",
    "\n",
    "-  Save the trained classifier to a file for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the model  is 1.00\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load the iris dataset\n",
    "iris = load_iris()\n",
    "\n",
    "# Split the dataset into a training set and a testing set\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.3, random_state=42)\n",
    "    \n",
    "# Train a linear SVM classifier on the training set\n",
    "clf = LinearSVC(random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels for the testing set\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Compute the accuracy of the model on the testing set\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"The accuracy of the model  is {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the model is 1.00\n",
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
      "[CV 1/5] END .C=0.1, gamma=scale, kernel=linear;, score=0.952 total time=   0.0s\n",
      "[CV 2/5] END .C=0.1, gamma=scale, kernel=linear;, score=0.905 total time=   0.0s\n",
      "[CV 3/5] END .C=0.1, gamma=scale, kernel=linear;, score=0.905 total time=   0.0s\n",
      "[CV 4/5] END .C=0.1, gamma=scale, kernel=linear;, score=1.000 total time=   0.0s\n",
      "[CV 5/5] END .C=0.1, gamma=scale, kernel=linear;, score=0.905 total time=   0.0s\n",
      "[CV 1/5] END ....C=0.1, gamma=scale, kernel=rbf;, score=0.952 total time=   0.0s\n",
      "[CV 2/5] END ....C=0.1, gamma=scale, kernel=rbf;, score=0.762 total time=   0.0s\n",
      "[CV 3/5] END ....C=0.1, gamma=scale, kernel=rbf;, score=0.857 total time=   0.0s\n",
      "[CV 4/5] END ....C=0.1, gamma=scale, kernel=rbf;, score=1.000 total time=   0.0s\n",
      "[CV 5/5] END ....C=0.1, gamma=scale, kernel=rbf;, score=0.762 total time=   0.0s\n",
      "[CV 1/5] END ..C=0.1, gamma=auto, kernel=linear;, score=0.952 total time=   0.0s\n",
      "[CV 2/5] END ..C=0.1, gamma=auto, kernel=linear;, score=0.905 total time=   0.0s\n",
      "[CV 3/5] END ..C=0.1, gamma=auto, kernel=linear;, score=0.905 total time=   0.0s\n",
      "[CV 4/5] END ..C=0.1, gamma=auto, kernel=linear;, score=1.000 total time=   0.0s\n",
      "[CV 5/5] END ..C=0.1, gamma=auto, kernel=linear;, score=0.905 total time=   0.0s\n",
      "[CV 1/5] END .....C=0.1, gamma=auto, kernel=rbf;, score=0.952 total time=   0.0s\n",
      "[CV 2/5] END .....C=0.1, gamma=auto, kernel=rbf;, score=0.762 total time=   0.0s\n",
      "[CV 3/5] END .....C=0.1, gamma=auto, kernel=rbf;, score=0.857 total time=   0.0s\n",
      "[CV 4/5] END .....C=0.1, gamma=auto, kernel=rbf;, score=1.000 total time=   0.0s\n",
      "[CV 5/5] END .....C=0.1, gamma=auto, kernel=rbf;, score=0.762 total time=   0.0s\n",
      "[CV 1/5] END ...C=1, gamma=scale, kernel=linear;, score=0.952 total time=   0.0s\n",
      "[CV 2/5] END ...C=1, gamma=scale, kernel=linear;, score=0.905 total time=   0.0s\n",
      "[CV 3/5] END ...C=1, gamma=scale, kernel=linear;, score=0.905 total time=   0.0s\n",
      "[CV 4/5] END ...C=1, gamma=scale, kernel=linear;, score=1.000 total time=   0.0s\n",
      "[CV 5/5] END ...C=1, gamma=scale, kernel=linear;, score=0.952 total time=   0.0s\n",
      "[CV 1/5] END ......C=1, gamma=scale, kernel=rbf;, score=0.952 total time=   0.0s\n",
      "[CV 2/5] END ......C=1, gamma=scale, kernel=rbf;, score=0.905 total time=   0.0s\n",
      "[CV 3/5] END ......C=1, gamma=scale, kernel=rbf;, score=0.905 total time=   0.0s\n",
      "[CV 4/5] END ......C=1, gamma=scale, kernel=rbf;, score=1.000 total time=   0.0s\n",
      "[CV 5/5] END ......C=1, gamma=scale, kernel=rbf;, score=0.952 total time=   0.0s\n",
      "[CV 1/5] END ....C=1, gamma=auto, kernel=linear;, score=0.952 total time=   0.0s\n",
      "[CV 2/5] END ....C=1, gamma=auto, kernel=linear;, score=0.905 total time=   0.0s\n",
      "[CV 3/5] END ....C=1, gamma=auto, kernel=linear;, score=0.905 total time=   0.0s\n",
      "[CV 4/5] END ....C=1, gamma=auto, kernel=linear;, score=1.000 total time=   0.0s\n",
      "[CV 5/5] END ....C=1, gamma=auto, kernel=linear;, score=0.952 total time=   0.0s\n",
      "[CV 1/5] END .......C=1, gamma=auto, kernel=rbf;, score=0.952 total time=   0.0s\n",
      "[CV 2/5] END .......C=1, gamma=auto, kernel=rbf;, score=0.905 total time=   0.0s\n",
      "[CV 3/5] END .......C=1, gamma=auto, kernel=rbf;, score=0.905 total time=   0.0s\n",
      "[CV 4/5] END .......C=1, gamma=auto, kernel=rbf;, score=1.000 total time=   0.0s\n",
      "[CV 5/5] END .......C=1, gamma=auto, kernel=rbf;, score=0.952 total time=   0.0s\n",
      "[CV 1/5] END ..C=10, gamma=scale, kernel=linear;, score=0.952 total time=   0.0s\n",
      "[CV 2/5] END ..C=10, gamma=scale, kernel=linear;, score=0.905 total time=   0.0s\n",
      "[CV 3/5] END ..C=10, gamma=scale, kernel=linear;, score=0.905 total time=   0.0s\n",
      "[CV 4/5] END ..C=10, gamma=scale, kernel=linear;, score=1.000 total time=   0.0s\n",
      "[CV 5/5] END ..C=10, gamma=scale, kernel=linear;, score=1.000 total time=   0.0s\n",
      "[CV 1/5] END .....C=10, gamma=scale, kernel=rbf;, score=0.952 total time=   0.0s\n",
      "[CV 2/5] END .....C=10, gamma=scale, kernel=rbf;, score=0.905 total time=   0.0s\n",
      "[CV 3/5] END .....C=10, gamma=scale, kernel=rbf;, score=0.905 total time=   0.0s\n",
      "[CV 4/5] END .....C=10, gamma=scale, kernel=rbf;, score=1.000 total time=   0.0s\n",
      "[CV 5/5] END .....C=10, gamma=scale, kernel=rbf;, score=0.952 total time=   0.0s\n",
      "[CV 1/5] END ...C=10, gamma=auto, kernel=linear;, score=0.952 total time=   0.0s\n",
      "[CV 2/5] END ...C=10, gamma=auto, kernel=linear;, score=0.905 total time=   0.0s\n",
      "[CV 3/5] END ...C=10, gamma=auto, kernel=linear;, score=0.905 total time=   0.0s\n",
      "[CV 4/5] END ...C=10, gamma=auto, kernel=linear;, score=1.000 total time=   0.0s\n",
      "[CV 5/5] END ...C=10, gamma=auto, kernel=linear;, score=1.000 total time=   0.0s\n",
      "[CV 1/5] END ......C=10, gamma=auto, kernel=rbf;, score=0.952 total time=   0.0s\n",
      "[CV 2/5] END ......C=10, gamma=auto, kernel=rbf;, score=0.905 total time=   0.0s\n",
      "[CV 3/5] END ......C=10, gamma=auto, kernel=rbf;, score=0.905 total time=   0.0s\n",
      "[CV 4/5] END ......C=10, gamma=auto, kernel=rbf;, score=1.000 total time=   0.0s\n",
      "[CV 5/5] END ......C=10, gamma=auto, kernel=rbf;, score=0.952 total time=   0.0s\n",
      "The best parameters are {'C': 10, 'gamma': 'scale', 'kernel': 'linear'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['svm_classifier.joblib']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the necessary libraries\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import joblib\n",
    "\n",
    "# Load the dataset\n",
    "iris = load_iris()\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.3, random_state=42)\n",
    "\n",
    "# Preprocess the data using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Create an instance of the SVC classifier and train it on the training data\n",
    "clf = SVC(kernel='rbf', C=1.0, gamma='scale', random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Use the trained classifier to predict the labels of the testing data\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Evaluate the performance of the classifier using accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"The accuracy of the model is {accuracy:.2f}\")\n",
    "\n",
    "# Tune the hyperparameters of the SVC classifier using GridSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = {'C': [0.1, 1, 10], 'gamma': ['scale', 'auto'], 'kernel': ['linear', 'rbf']}\n",
    "grid = GridSearchCV(SVC(), param_grid, refit=True, verbose=3)\n",
    "grid.fit(X_train, y_train)\n",
    "print(f\"The best parameters are {grid.best_params_}\")\n",
    "\n",
    "# Train the tuned classifier on the entire dataset\n",
    "clf_tuned = SVC(**grid.best_params_)\n",
    "clf_tuned.fit(iris.data, iris.target)\n",
    "\n",
    "# Save the trained classifier to a file for future use\n",
    "joblib.dump(clf_tuned, 'svm_classifier.joblib')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
