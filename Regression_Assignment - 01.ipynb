{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1. Explain the difference between simple linear regression and multiple linear regression. Provide an example of each."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Simple linear regression** is a statistical method used to model the relationship between two variables, where one variable is the independent variable and the other is the dependent variable. The goal of simple linear regression is to find a line that best fits the data points. The equation for simple linear regression is `y = b0 + b1*x`, where `y` is the dependent variable, `x` is the independent variable, `b0` is the y-intercept, and `b1` is the slope of the line.\n",
    "## An example of simple linear regression would be predicting a student's test score based on the number of hours they studied.\n",
    "\n",
    "**Multiple linear regression** is a statistical method used to model the relationship between more than two variables, where one variable is the dependent variable and the others are independent variables. The goal of multiple linear regression is to find a line that best fits the data points. The equation for multiple linear regression is `y = b0 + b1*x1 + b2*x2 + ... + bn*xn`, where `y` is the dependent variable, `x1`, `x2`, ..., `xn` are independent variables, and `b0`, `b1`, `b2`, ..., `bn` are coefficients.\n",
    "## An example of multiple linear regression would be predicting a person's salary based on their age, education level, and years of experience."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2. Discuss the assumptions of linear regression. How can you check whether these assumptions hold in a given dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The assumptions of linear regression are as follows:\n",
    "\n",
    "1. **Linearity**: The relationship between the dependent variable and the independent variables should be linear. This means that the change in the dependent variable is proportional to the change in the independent variable(s). You can check this assumption by creating a scatter plot of the dependent variable against each independent variable. If the plot shows a linear pattern, then this assumption is met.\n",
    "\n",
    "2. **Independence**: The observations should be independent of each other. This means that there should be no relationship between the residuals (the difference between the predicted value and the actual value) of one observation and the residuals of any other observation. You can check this assumption by creating a scatter plot of the residuals against each independent variable. If there is no pattern in the plot, then this assumption is met.\n",
    "\n",
    "3. **Homoscedasticity**: The variance of the residuals should be constant across all levels of the independent variables. This means that the spread of the residuals should be roughly equal throughout the range of values for each independent variable. You can check this assumption by creating a scatter plot of the residuals against each predicted value. If there is no pattern in the plot, then this assumption is met.\n",
    "\n",
    "4. **Normality**: The residuals should be normally distributed. This means that they should follow a bell-shaped curve when plotted on a histogram or a normal probability plot. You can check this assumption by creating a histogram or a normal probability plot of the residuals. If they follow a bell-shaped curve, then this assumption is met.\n",
    "\n",
    "5. **No multicollinearity**: The independent variables should not be highly correlated with each other. This means that one independent variable should not be a perfect predictor of another independent variable. You can check this assumption by calculating the correlation coefficient between each pair of independent variables. If any pair has a correlation coefficient greater than 0.8, then this assumption is not met.\n",
    "\n",
    "6. **Number of observations**: The number of observations should be greater than the number of predictors (independent variables). This means that you need enough data to estimate all of the coefficients in your model with reasonable accuracy.\n",
    "\n",
    "7. **Each observation is unique**: Each observation in your dataset should be unique and not a duplicate of any other observation.\n",
    "\n",
    "To check whether these assumptions hold in a given dataset, you can create diagnostic plots such as scatter plots, histograms, and normal probability plots to visually inspect your data for patterns that violate these assumptions ¹²³. You can also use statistical tests such as the Durbin-Watson test for autocorrelation and tests for normality to formally test these assumptions ¹²."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3. How do you interpret the slope and intercept in a linear regression model? Provide an example using a real-world scenario."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a linear regression model, the **slope** represents the change in the dependent variable for a one-unit increase in the independent variable. The **intercept** represents the value of the dependent variable when the independent variable is zero ¹².\n",
    "\n",
    "For example, suppose we want to predict a person's salary based on their years of experience. We collect data from 100 employees and fit a linear regression model with years of experience as the independent variable and salary as the dependent variable. The equation for this model is `salary = 30,000 + 5,000*years_of_experience`. In this model, the **intercept** is 30,000, which means that an employee with zero years of experience would have a predicted salary of 30,000 dollars. The **slope** is 5,000, which means that for every additional year of experience, an employee's predicted salary increases by 5,000 dollars."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q4. Explain the concept of gradient descent. How is it used in machine learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gradient Descent** is an optimization algorithm that's commonly used in machine learning and deep learning¹²³⁴⁵. It's used to find the values of a function's parameters (coefficients) that minimize a cost function as far as possible³.\n",
    "\n",
    "The concept of gradient descent was initially discovered by Augustin-Louis Cauchy in the mid of the 18th century¹. It helps in finding the local minimum of a function¹. The best way to define the local minimum or local maximum of a function using gradient descent is as follows:\n",
    "\n",
    "- If we move towards a negative gradient or away from the gradient of the function at the current point, it will give the local minimum of that function¹.\n",
    "- Whenever we move towards a positive gradient or towards the gradient of the function at the current point, we will get the local maximum of that function¹.\n",
    "\n",
    "This entire procedure is known as Gradient Ascent, which is also known as steepest descent¹.\n",
    "\n",
    "The main objective of using a gradient descent algorithm is to minimize the cost function using iteration¹. To achieve this goal, it performs two steps iteratively:\n",
    "\n",
    "1. Calculates the first-order derivative of the function to compute the gradient or slope of that function¹.\n",
    "2. Move away from the direction of the gradient, which means slope increased from the current point by alpha times, where Alpha is defined as Learning Rate¹. It is a tuning parameter in the optimization process which helps to decide the length of the steps¹.\n",
    "\n",
    "The **cost function** is defined as the measurement of difference or error between actual values and expected values at the current position and present in the form of a single real number¹. It helps to increase and improve machine learning efficiency by providing feedback to this model so that it can minimize error and find the local or global minimum¹. Further, it continuously iterates along the direction of the negative gradient until the cost function approaches zero¹."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q5. Describe the multiple linear regression model. How does it differ from simple linear regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Multiple Linear Regression (MLR)** is a statistical technique that uses several explanatory variables to predict the outcome of a response variable⁵⁷⁸⁹. It is used to estimate the relationship between two or more independent variables and one dependent variable⁵. The goal of MLR is to model the linear relationship between the explanatory (independent) variables and response (dependent) variable⁷.\n",
    "\n",
    "The formula for a multiple linear regression is:\n",
    "\n",
    "$$y = b_0 + b_1x_1 + b_2x_2 + ... + b_nx_n$$\n",
    "\n",
    "where:\n",
    "- $y$ is the predicted value of the dependent variable\n",
    "- $b_0$ is the y-intercept (value of $y$ when all other parameters are set to 0)\n",
    "- $b_i$ are the regression coefficients of the independent variables $x_i$⁵.\n",
    "\n",
    "Multiple linear regression makes all of the same assumptions as simple linear regression: homogeneity of variance (homoscedasticity), independence of observations, normality, and linearity⁵. In multiple linear regression, it is possible that some of the independent variables are actually correlated with one another, so it's important to check these before developing the regression model⁵.\n",
    "\n",
    "Now, comparing **Multiple Linear Regression** with **Simple Linear Regression**:\n",
    "\n",
    "- **Simple Linear Regression** establishes a relationship between two variables. It is graphically depicted using a straight line with the slope defining how the change in one variable impacts a change in the other¹. In simple linear regression, every dependent value has a single corresponding independent variable that drives its value¹.\n",
    "- On the other hand, **Multiple Linear Regression** incorporates multiple independent variables. Each independent variable in multiple regression has its own coefficient to ensure each variable is weighted appropriately¹. For straightforward relationships, simple linear regression may easily capture the relationship between two variables. For more complex relationships requiring more consideration, multiple linear regression is often better¹."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q6. Explain the concept of multicollinearity in multiple linear regression. How can you detect and address this issue?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Multicollinearity** is a phenomenon in which one independent variable in a multiple regression model can be linearly predicted from one or more of the other independent variables¹². It occurs when two or more independent variables are highly correlated with one another¹². This correlation is a problem because independent variables should be independent². If the degree of correlation between variables is high enough, it can cause problems when you fit the model and interpret the results².\n",
    "\n",
    "Multicollinearity can cause two primary issues:\n",
    "1. The coefficient estimates can swing wildly based on which other independent variables are in the model. The coefficients become very sensitive to small changes in the model².\n",
    "2. Multicollinearity reduces the precision of the estimated coefficients, which weakens the statistical power of your regression model².\n",
    "\n",
    "To **detect multicollinearity**, you can use:\n",
    "1. **Variance Inflation Factor (VIF)**: VIF is a measure of collinearity among predictor variables within a multiple regression. It is calculated by taking the ratio of the variance of all a given model’s betas to divide by the variance of a single beta if it were fit alone¹.\n",
    "2. **Heat map or correlation matrix**: These are graphical representations that can help identify if multicollinearity exists in your data¹.\n",
    "\n",
    "To **address multicollinearity**, you can:\n",
    "1. Omit variables that have a high VIF value⁷.\n",
    "2. Replace outlier data with new data from the field (in cross-sectional data)⁷.\n",
    "3. Add or subtract the number of observations⁷.\n",
    "4. Perform variable transformation⁷.\n",
    "5. Remove one or more variables showing a high correlation⁸."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q7. Describe the polynomial regression model. How is it different from linear regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Polynomial Regression** is a type of regression analysis that models the relationship between a dependent (y) and independent variable (x) as an nth degree polynomial¹. It's used when the relationship between the dependent and independent variables is not linear¹². The equation for a polynomial regression model can be written as:\n",
    "\n",
    "$$y = b_0 + b_1x + b_2x^2 + ... + b_nx^n$$\n",
    "\n",
    "where:\n",
    "- $y$ is the predicted value of the dependent variable\n",
    "- $b_0$ is the y-intercept\n",
    "- $b_i$ are the regression coefficients of the independent variables $x_i$¹.\n",
    "\n",
    "Polynomial regression fits a nonlinear relationship between the value of x and the corresponding conditional mean of y, denoted E(y|x)¹². It makes use of a linear regression model to fit complicated and non-linear functions and datasets¹.\n",
    "\n",
    "Now, comparing **Polynomial Regression** with **Linear Regression**:\n",
    "\n",
    "- **Linear Regression** assumes a linear relationship between a dependent variable (Y) and an independent variable (X). It uses a straight line to represent this relationship. The equation for a simple linear regression model can be written as: $y = b_0 + b_1x$⁶.\n",
    "- On the other hand, **Polynomial Regression** models the non-linear relationship between a predictor and an outcome variable using the Nth-degree polynomial of the predictor⁴. It involves fitting a polynomial function to the data points to obtain a curve that represents the relationship between the variables⁶.\n",
    "\n",
    "In essence, Polynomial Regression is considered a special case of Multiple Linear Regression. But instead of fitting a straight line to the data, Polynomial Regression fits a nth degree polynomial, providing a better fit for non-linear data¹⁶."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q8. What are the advantages and disadvantages of polynomial regression compared to linear regression? In what situations would you prefer to use polynomial regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Advantages of Polynomial Regression**:\n",
    "1. Polynomial regression can model non-linear relationships⁴.\n",
    "2. It is more flexible than linear regression⁴.\n",
    "3. It can cover more data points and improve the results to a considerable amount³.\n",
    "\n",
    "**Disadvantages of Polynomial Regression**:\n",
    "1. It requires some knowledge of data to select the best exponents⁴.\n",
    "2. It is prone to overfitting if exponents are poorly selected⁴.\n",
    "3. If the relationship is actually linear, using polynomial regression might increase the loss function and the error rate, and decrease the accuracy of the model⁵.\n",
    "\n",
    "Compared to **Linear Regression**, Polynomial Regression is able to model non-linearly separable data and is much more flexible⁴. However, if the relationship takes after a curved or complex pattern, linear regression may not capture it accurately⁵. In such cases, alternative models, such as polynomial regression or non−linear regression, may be more suitable to capture the basic structure of the information⁵.\n",
    "\n",
    "Polynomial Regression is preferred in situations where the relationship between the predictor variable(s) and the response variable is non-linear⁶⁷[^10^]. Here are some ways to determine if you should use polynomial regression:\n",
    "\n",
    "1. **Create a Scatterplot**: If the scatterplot of the predictor variable and the response variable shows a non-linear relationship, then it may be a good idea to fit a polynomial regression model⁶⁷.\n",
    "2. **Create a Fitted Values vs. Residual Plot**: If there is a clear non-linear pattern in the residuals after fitting a linear regression model, then this is an indication that polynomial regression could offer a better fit to the data⁶⁷.\n",
    "3. **Calculate the Adjusted R-Squared Value**: If you fit both a linear regression model and a polynomial regression model and calculate the adjusted R-squared values for both models, the model with the higher adjusted R-squared represents the model that is better able to use the predictor variable(s) to explain the variation in the response variable⁶.\n",
    "\n",
    "Some use cases of Polynomial Regression include studying the growth rate of tissues, progression of disease epidemics, distribution of carbon isotopes in lake sediments⁸, and studying isotopes of sediments⁹."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
