{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q1. A company conducted a survey of its employees and found that 70% of the employees use the company's health insurance plan, while 40% of the employees who use the plan are smokers. What is the probability that an employee is a smoker given that he/she uses the health insurance plan?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To solve this problem, we can use Bayes' theorem. Let A be the event that an employee is a smoker and B be the event that an employee uses the company's health insurance plan. We want to find the probability of A given B, i.e., P(A|B).\n",
    "\n",
    "We know that P(B|A) = 0.4, which is the probability of an employee using the health insurance plan given that the employee is a smoker. We also know that P(B) = 0.7, which is the probability of an employee using the health insurance plan. We want to find P(A|B), which is the probability of an employee being a smoker given that the employee uses the health insurance plan.\n",
    "\n",
    "Using Bayes' theorem, we have:\n",
    "\n",
    "P(A|B) = P(B|A) * P(A) / P(B)\n",
    "\n",
    "Substituting the given values, we get:\n",
    "\n",
    "P(A|B) = 0.4 * P(A) / 0.7\n",
    "\n",
    "We know that P(A) + P(~A) = 1, where ~A is the complement of A. Since we don't have any information about the probability of an employee not being a smoker, we can assume that P(~A) = 1 - P(A) = 0.5. Therefore, we have:\n",
    "\n",
    "P(A) = 0.5\n",
    "\n",
    "Substituting this value, we get:\n",
    "\n",
    "P(A|B) = 0.4 * 0.5 / 0.7\n",
    "\n",
    "Simplifying, we get:\n",
    "\n",
    "P(A|B) = 0.286\n",
    "\n",
    "Therefore, the probability that an employee is a smoker given that he/she uses the health insurance plan is **0.286**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q2. What is the difference between Bernoulli Naive Bayes and Multinomial Naive Bayes?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bernoulli Naive Bayes and Multinomial Naive Bayes are two variants of the Naive Bayes algorithm that are used for classification tasks. The main difference between the two is the type of data they are designed to handle ¹.\n",
    "\n",
    "Bernoulli Naive Bayes is used for **discrete data** where the features are only in binary form (i.e., 0 or 1) ¹. It is commonly used in text classification problems where the presence or absence of a word is used as a feature ¹. Bernoulli Naive Bayes explicitly models the presence/absence of a feature ³.\n",
    "\n",
    "Multinomial Naive Bayes, on the other hand, is used for **discrete data** where the features are counts (i.e., integer values) ¹. It is commonly used in text classification problems where the frequency of a word is used as a feature ¹. Multinomial Naive Bayes cares about counts for multiple features that do occur ³.\n",
    "\n",
    "In summary, Bernoulli Naive Bayes is used when the features are binary, while Multinomial Naive Bayes is used when the features are counts ¹. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q3. How does Bernoulli Naive Bayes handle missing values?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When constructing probability tables for Bernoulli Naive Bayes, **missing values (NAs) are omitted** ¹. The corresponding predict function excludes all NAs from the calculation of posterior probabilities ¹. When training a naive Bayes classifier, you can choose to either omit records with any missing values or omit only the missing attributes ¹. Another way to deal with missing values is to ignore that category if it is a missing value when calculating probabilities ¹. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q4. Can Gaussian Naive Bayes be used for multi-class classification?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, Gaussian Naive Bayes can be used for multi-class classification ¹. In scikit-learn, the GaussianNB class implements the Gaussian Naive Bayes algorithm for classification tasks ¹. It can be used for both binary and multi-class classification problems ¹. \n",
    "\n",
    "In multi-class classification, the algorithm estimates the probability of each class using the Gaussian distribution and then selects the class with the highest probability ². \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q5. Assignment:\n",
    "\n",
    "- Data preparation:\n",
    "\n",
    "Download the \"Spambase Data Set\" from the UCI Machine Learning Repository (https://archive.ics.uci.edu/ml/\n",
    "datasets/Spambase). This dataset contains email messages, where the goal is to predict whether a message\n",
    "is spam or not based on several input features.\n",
    "\n",
    "- Implementation:\n",
    "\n",
    "Implement Bernoulli Naive Bayes, Multinomial Naive Bayes, and Gaussian Naive Bayes classifiers using the\n",
    "scikit-learn library in Python. Use 10-fold cross-validation to evaluate the performance of each classifier on the\n",
    "dataset. You should use the default hyperparameters for each classifier.\n",
    "\n",
    "- Results:\n",
    "\n",
    "Report the following performance metrics for each classifier:\n",
    "Accuracy\n",
    "Precision\n",
    "Recall\n",
    "F1 score\n",
    "\n",
    "- Discussion:\n",
    "\n",
    "Discuss the results you obtained. Which variant of Naive Bayes performed the best? Why do you think that is\n",
    "the case? Are there any limitations of Naive Bayes that you observed?\n",
    "Conclusion:\n",
    "Summarise your findings and provide some suggestions for future work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To implement Bernoulli Naive Bayes, Multinomial Naive Bayes, and Gaussian Naive Bayes classifiers using the scikit-learn library in Python, you can follow these steps:\n",
    "\n",
    "1. Download the Spambase dataset from the UCI Machine Learning Repository.\n",
    "2. Load the dataset into Python using pandas.\n",
    "3. Split the dataset into training and testing sets.\n",
    "4. Train the Bernoulli Naive Bayes, Multinomial Naive Bayes, and Gaussian Naive Bayes classifiers using the training set.\n",
    "5. Evaluate the performance of each classifier using 10-fold cross-validation on the testing set.\n",
    "6. Report the following performance metrics for each classifier: Accuracy, Precision, Recall, and F1 score.\n",
    "\n",
    "Here are some code snippets to get you started:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bernoulli Naive Bayes:\n",
      "Accuracy: 0.8805646036916395\n",
      "Precision: 0.9069767441860465\n",
      "Recall: 0.8\n",
      "F1 score: 0.8501362397820164\n",
      "Cross-validation scores: [0.88172043 0.84782609 0.81521739 0.91304348 0.90217391 0.88043478\n",
      " 0.93478261 0.88043478 0.92391304 0.89130435]\n",
      "\n",
      "Multinomial Naive Bayes:\n",
      "Accuracy: 0.7861020629750272\n",
      "Precision: 0.7643835616438356\n",
      "Recall: 0.7153846153846154\n",
      "F1 score: 0.7390728476821192\n",
      "Cross-validation scores: [0.79569892 0.7173913  0.79347826 0.73913043 0.83695652 0.84782609\n",
      " 0.81521739 0.79347826 0.80434783 0.80434783]\n",
      "\n",
      "Gaussian Naive Bayes:\n",
      "Accuracy: 0.8208469055374593\n",
      "Precision: 0.7192982456140351\n",
      "Recall: 0.9461538461538461\n",
      "F1 score: 0.8172757475083057\n",
      "Cross-validation scores: [0.79569892 0.79347826 0.86956522 0.85869565 0.86956522 0.83695652\n",
      " 0.85869565 0.83695652 0.84782609 0.80434783]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB, GaussianNB\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('spambase.data', header=None)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(data.iloc[:, :-1], data.iloc[:, -1], test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the Bernoulli Naive Bayes classifier\n",
    "bnb = BernoulliNB()\n",
    "bnb.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the performance of the Bernoulli Naive Bayes classifier\n",
    "bnb_scores = cross_val_score(bnb, X_test, y_test, cv=10)\n",
    "bnb_accuracy = accuracy_score(y_test, bnb.predict(X_test))\n",
    "bnb_precision = precision_score(y_test, bnb.predict(X_test))\n",
    "bnb_recall = recall_score(y_test, bnb.predict(X_test))\n",
    "bnb_f1_score = f1_score(y_test, bnb.predict(X_test))\n",
    "\n",
    "# Train the Multinomial Naive Bayes classifier\n",
    "mnb = MultinomialNB()\n",
    "mnb.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the performance of the Multinomial Naive Bayes classifier\n",
    "mnb_scores = cross_val_score(mnb, X_test, y_test, cv=10)\n",
    "mnb_accuracy = accuracy_score(y_test, mnb.predict(X_test))\n",
    "mnb_precision = precision_score(y_test, mnb.predict(X_test))\n",
    "mnb_recall = recall_score(y_test, mnb.predict(X_test))\n",
    "mnb_f1_score = f1_score(y_test, mnb.predict(X_test))\n",
    "\n",
    "# Train the Gaussian Naive Bayes classifier\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the performance of the Gaussian Naive Bayes classifier\n",
    "gnb_scores = cross_val_score(gnb, X_test, y_test, cv=10)\n",
    "gnb_accuracy = accuracy_score(y_test, gnb.predict(X_test))\n",
    "gnb_precision = precision_score(y_test, gnb.predict(X_test))\n",
    "gnb_recall = recall_score(y_test, gnb.predict(X_test))\n",
    "gnb_f1_score = f1_score(y_test, gnb.predict(X_test))\n",
    "\n",
    "# Report the performance metrics for each classifier\n",
    "print('Bernoulli Naive Bayes:')\n",
    "print('Accuracy:', bnb_accuracy)\n",
    "print('Precision:', bnb_precision)\n",
    "print('Recall:', bnb_recall)\n",
    "print('F1 score:', bnb_f1_score)\n",
    "print('Cross-validation scores:', bnb_scores)\n",
    "print()\n",
    "print('Multinomial Naive Bayes:')\n",
    "print('Accuracy:', mnb_accuracy)\n",
    "print('Precision:', mnb_precision)\n",
    "print('Recall:', mnb_recall)\n",
    "print('F1 score:', mnb_f1_score)\n",
    "print('Cross-validation scores:', mnb_scores)\n",
    "print()\n",
    "print('Gaussian Naive Bayes:')\n",
    "print('Accuracy:', gnb_accuracy)\n",
    "print('Precision:', gnb_precision)\n",
    "print('Recall:', gnb_recall)\n",
    "print('F1 score:', gnb_f1_score)\n",
    "print('Cross-validation scores:', gnb_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, the performance of each variant of Naive Bayes depends on the nature of the data and the problem at hand. Bernoulli Naive Bayes is used for binary data, Multinomial Naive Bayes is used for discrete data, and Gaussian Naive Bayes is used for continuous data. \n",
    "\n",
    "One limitation of Naive Bayes is that it assumes that the features are independent, which may not always be the case in real-world problems."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
